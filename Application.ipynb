{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils import *\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 167s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_x_orig, train_y), (test_x_orig, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000 [[6]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [1]\n",
      " [1]]\n",
      "train_y[index][0] 9\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "y = 9. It's a truck picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 50\n",
    "plt.imshow(train_x_orig[index])\n",
    "print(len(train_x_orig), len(train_y), train_y)\n",
    "print('train_y[index][0]', train_y[index][0])\n",
    "print(classes)\n",
    "print (\"y = \" + str(train_y[index][0]) + \". It's a \" + classes[train_y[index][0]] + \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 50000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (32, 32, 3)\n",
      "train_x_orig shape: (50000, 32, 32, 3)\n",
      "train_y shape: (50000, 1)\n",
      "test_x_orig shape: (10000, 32, 32, 3)\n",
      "test_y shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (3072, 50000)\n",
      "test_x's shape: (3072, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten / 255.\n",
    "test_x = test_x_flatten / 255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "# https://www.samyzaf.com/ML/cifar10/cifar10.html\n",
    "# The CIFAR-10 dataset consists of 60000 32x32x3 color images in 10 equal classes, (6000 images per class)\n",
    "n_x = 32 * 32 * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        ###\n",
    "        if print_cost and i < 3:\n",
    "            print()\n",
    "            print(\"iteration {}\".format(i))\n",
    "            print('W1, W2', W1, W2)\n",
    "            print('b1, b2', b1, b2)\n",
    "            print('A1, A2', A1, A2)\n",
    "        ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        ###\n",
    "        if print_cost and i < 3:\n",
    "            print('cost', cost)\n",
    "        ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        if print_cost and i < 3:\n",
    "            print('dA2', dA2)\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        if print_cost and i < 3:\n",
    "            print('dA1, dW2, db2', dA1, dW2, db2)\n",
    "            print('dA0, dW1, db1', dA0, dW1, db1)\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i < 3:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: three_layer_model\n",
    "\n",
    "def three_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, W3, b1, b2 and b3\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'relu')\n",
    "        A3, cache3 = linear_activation_forward(A2, W3, b3, 'sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        ###\n",
    "        if print_cost and i < 3:\n",
    "            print()\n",
    "            print(\"iteration {}\".format(i))\n",
    "            print('W1, W2, W3', W1, W2, W3)\n",
    "            print('b1, b2, b3', b1, b2, b3)\n",
    "            print('A1, A2, A3', A1, A2, A3)\n",
    "        ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A3, Y)\n",
    "        #cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        ###\n",
    "        if print_cost and i < 3:\n",
    "            print('cost', cost)\n",
    "        ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
    "#         dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        if print_cost and i < 3:\n",
    "            print('dA3', dA3)\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA2, dW3, db3 = linear_activation_backward(dA3, cache3, 'sigmoid')\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'relu')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        if print_cost and i < 3:\n",
    "            print('dA2, dW3, db3', dA2, dW3, db3)\n",
    "            print('dA1, dW2, db2', dA1, dW2, db2)\n",
    "            print('dA0, dW1, db1', dA0, dW1, db1)\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        grads['dW3'] = dW3\n",
    "        grads['db3'] = db3\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        W3 = parameters[\"W3\"]\n",
    "        b3 = parameters[\"b3\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i < 3:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 0\n",
      "W1, W2 [[ 0.01624345 -0.00611756 -0.00528172 ...  0.01207317  0.01377667\n",
      "   0.00636284]\n",
      " [ 0.00055297  0.00405817  0.01248457 ... -0.01951761  0.00212729\n",
      "   0.00108224]\n",
      " [ 0.00131307 -0.01363814  0.00239215 ... -0.00678362 -0.01135211\n",
      "   0.00050318]\n",
      " ...\n",
      " [ 0.01009231  0.00229889 -0.00664099 ... -0.01057715  0.0050263\n",
      "   0.00554066]\n",
      " [-0.00908842 -0.01518325 -0.00132815 ... -0.00356188  0.00250796\n",
      "   0.00541891]\n",
      " [-0.00195132 -0.0134674  -0.00570327 ...  0.00961913  0.00877438\n",
      "   0.01493611]] [[-0.003477   -0.00514324 -0.00085924 -0.0114153   0.00056509  0.00261963\n",
      "   0.00617491]]\n",
      "b1, b2 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] [[0.]]\n",
      "A1, A2 [[0.3104337  0.31851562 0.5136688  ... 0.51513792 0.59188517 0.49182025]\n",
      " [0.02115502 0.09622938 0.         ... 0.         0.05450168 0.        ]\n",
      " [0.40891575 0.01099054 0.         ... 0.         0.         0.18153547]\n",
      " ...\n",
      " [0.35653334 0.23111246 0.         ... 0.         0.         0.26021737]\n",
      " [0.         0.13805693 0.         ... 0.         0.06232653 0.01177826]\n",
      " [0.         0.         0.09008326 ... 0.         0.         0.        ]] [[0.49900211 0.49916924 0.4982922  ... 0.49849258 0.49798227 0.49859507]]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a1bdb2c471e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwo_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6536aa4e7ee3>\u001b[0m in \u001b[0;36mtwo_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# Compute cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\deep-learning-coursera\\Neural Networks and Deep Learning\\dnn_app_utils.py\u001b[0m in \u001b[0;36mcompute_cost\u001b[1;34m(AL, Y)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;31m# Compute loss from aL and y.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m### START CODE HERE ### (≈ 1 lines of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 10, print_cost=True)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = three_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 10, print_cost=True)\n",
    "print(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
